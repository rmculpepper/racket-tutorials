#lang scribble/manual
@(require (for-label racket syntax/parse)
          scribble/example
          "../util.rkt")

@title[#:tag "macro-ll1"]{Macro Tutorial: LL(1) Parser Generator}
@author[@author+email["Ryan Culpepper" "ryanc@racket-lang.org"]]

@(begin
  (define (this-repo-url subpath)
    (format "https://github.com/rmculpepper/racket-tutorials/tree/master/~a" subpath))
  (define (this-repo-link subpath)
    (url (this-repo-url subpath)))
  (define (this-repo-hyperlink subpath)
    (hyperlink (this-repo-url subpath) (tt subpath)))
  (define (mk-file-url path start-line)
    (hyperlink (this-repo-url (format "parser/~a#L~a" path start-line)) (tt path)))
  (define-syntax-rule (include-code path label)
    (include-code-block path label mk-file-url)))

This tutorial uses the example of an LL(1) parser generator to demonstrate
Racket macros and macro-related techniques.

@emph{You don't need to know anything about LL(1) parsing in order to read the
tutorial.} You can just treat the LL(1) table generation as an opaque function;
the important parts are generating the inputs and using the outputs.

This tutorial's source code lives at

@centered{@this-repo-link{parser}}

That directory contains the Scribble source for this document and also complete,
working, tested snapshots of the each version of the code discussed below.


@; ============================================================
@section{Version 1: Parsing by Interpreting LL(1) Tables}

@margin-note{The examples for this section live in
@this-repo-hyperlink{parser/v1}.}

Let's implement an LL(1) parser generator. In version 1, we'll do that in
ordinary Racket without macros.

@subsection{Parsers and Parser Generators}

In this tutorial, by @emph{parser} I mean a function that takes a linear
sequence of @emph{tokens} and turns it into a structured syntax tree. A token
consists of a symbol that says what kind of token it is (like @racket['number]
or @racket['lparen]) and a payload that carries the token's content (if it has
content that we care about). For simplicity, we'll write tokens with payloads as
pairs, like @racket['(number . 5)], and we'll write tokens with unimportant
payloads as plain symbols, like @racket['lparen]. For example, a parser for
S-expressions would turn a list of tokens like

@racketblock[
'(lparen (symbol . "println") (string . "hello world") rparen)
]
into the S-expression value
@racket[
'(println "hello world")
]

Here is the data definition for @tt{Token} and some useful functions:

@include-code["v1/parser.rkt" "token"]

(We'll define @tt{Terminal} shortly.)

A @emph{parser generator} takes a specification of a grammar such as

@verbatim{
S  ::= lparen SL rparen
     | symbol
     | string
SL ::= ε
     | S SL
}

and produces a parser function. The grammar contains @emph{definitions} of
@emph{nonterminals}; each definition consists of one or more
@emph{productions}. In the example above, the nonterminals are @tt{S} and
@tt{SL}; there are four @tt{S} productions and two @tt{SL} productions. A
production contains a sequence of nonterminals and @emph{terminals}; a terminal
refers to a kind of token. The first @tt{S} production can be read as follows:
``An @tt{S} can be generated by concatenating an @tt{lparen} token, any complete
sequence of tokens generated by @tt{SL}, and an @tt{rparen} token.''

The grammar above describes the structure of S-expressions (rather, a simple
subset thereof), but it doesn't describe what the parser should do when it
recognizes that structure in a token sequence. To specify that, a parser
generator needs a grammar augmented by an action routine for each
production. For example:

@verbatim{
S  ::= lparen SL rparen  => (lambda (lp sl rp) sl)
     | symbol            => (lambda (sym) sym)
     | string            => (lambda (str) str)
SL ::= ε                 => (lambda () null)
     | S SL              => (lambda (s sl) (cons s sl))
}

To parse an @tt{S}, we first peek at the next token, and then we apply the
following rules:
@itemlist[
@item{If it is an @tt{lparen}, we must use the first @tt{S} production. We want
to parse an @tt{lparen}, then a @tt{SL}, then a @tt{rparen}.}
@item{If the next token is a @tt{symbol} or @tt{string}, we use the second or
third production, respectively.}
]

Once we've chosen a production, we parse each of its @emph{elements} in
order. Each element consumes zero or more tokens and produces a result; the
result of a terminal is the token's payload, and the result of a nonterminal is
the result of the nonterminal's chosen action routine. We accumulate the result
and pass the rest of the tokens to the subsequent element. At the end, we apply
the action routine to the accumulated values, and we return its result and the
rest of the tokens.

Parsing an @tt{SL} is slightly trickier, but we can see that @tt{SL} only occurs
in the first production of @tt{S}, right before @tt{rparen}. So if we want to
parse an @tt{SL} and the next token is an @tt{rparen}, then we must have reached
the end. (We've peeked beyond the end of ``our'' part of the input, but that's
okay.) The rules are the following:
@itemlist[
@item{If the next token is @tt{rparen}, then we must use the first production,
which is empty, so we just call the action routine with no arguments.}
@item{If the next token is @tt{lparen}, @tt{symbol}, or @tt{string}, then that's
a valid starting token for an @tt{S}, so we use the second production.}
]

In fact, we can follow that approach for any grammar @emph{if} we have a table
that tells us, from peeking at the next token, exactly one production that we
must try to parse. Alas, it is not possible to build such a table for every
grammar. But if we can, then the grammar is called @emph{LL(1)}. For our parser,
we'll represent the table using nested hash tables:

@include-code["v1/parser.rkt" "LL1 table"]

Here are the definitions of @tt{Production}, @tt{NT}, and so on:
@margin-note{Prefab structs are transparent, so @racket[equal?] and hashing work
automatically. Prefab structs also have advantages related to macros, which
we'll talk about later.}

@include-code["v1/parser.rkt" "production types"]

Given the start symbol, the LL(1) table, and a list of tokens, it is
straightforward to parse the input by following the approach described
above. I'll summarize the function as follows: @margin-note{The code's filename
is a link that takes you to the full function definition.}

@include-code["v1/parser.rkt" "parser"]

So if we have an LL(1) table, we can run our generic, table-driven parser. But
how do we get the LL(1) table? There's an algorithm that computes it from the
grammar, which I will just summarize as the following:

@include-code["v1/parser.rkt" "make-ll1-table"]

A @tt{Grammar} just bundles up the grammar's @emph{start nonterminal} with the
nonterminal definitions:

@include-code["v1/parser.rkt" "grammar types"]

And that's the structure of an LL(1) parser generator. Although, really what
we're generating is the table that drives the parser, not the parser code per
se. We use the same interpreter for different LL(1) tables.

Let's test it on a simple example grammar (even simpler than the S-expression
grammar above):

@(begin
  (define v1-eval (make-base-eval))
  (v1-eval '(require "v1/parser.rkt")))

@include-examples["v1/test.rkt" "test-arith" mk-file-url [#:eval v1-eval]]

@(close-eval v1-eval)

The good news is that it works. The bad news is that the interface is
awkward. In particular, there are three main reasons to be dissatisfied with the
code so far:
@itemlist[

@item{The way we specify a grammar is verbose. The grammar AST is convenient for
writing the table-generation algorithm and the table-driven parser, but it is
cumbersome to write grammar ASTs by hand. We'd prefer a concise grammar
@emph{notation} that gets @emph{validated} and turned into an AST
automatically. Validation should check, for example, that the start symbol is
defined as a nonterminal and only symbols are used in the right-hand sides of
productions.}

@item{We would also like a better notation for action routines. For example, we
could give each element a name in the production, or a name could be generated
automatically.  (This could be considered part of the first point, but I'll
address it separately.)}

@item{Finally, we generally expect to know the grammar at compile time, and so
we'd like to do the work of computing the LL(1) table at compile time.}
]

The rest of this tutorial will address these three points using Racket's macro
system.


@; ============================================================
@section{Version 2: Improving the front end}

@; ----------------------------------------
@subsection{An S-expression front end}

@;{
add S-expression front end that is "parsed" (ouch) to AST
- When is a grammar syntactically well-formed?
- Maybe pre-process a little?
  - wf checks
  - disambiguate ntelem vs telem
  - def list to hash, prod struct
}

@; ----------------------------------------
@subsection{A basic macro front end}

@;{
2a. syntax classes, just data language ("eliminate quotes")
- OLD: parse to data structure, NEW: parse to expr to make data structure
}

@; ----------------------------------------
@subsection{Compile-time context-sensitive checks}

@;{
2b. now check wf at compile time
- two approaches: multi-pass parsing vs HO attributes
}

@; ----------------------------------------
@subsection{Make action routines nicer}

@;{
2c. revise syntax, make action routines nicer
}

@; ============================================================
@section{Version 3: Moving computation to compile time}

@;{
3. want to compute First/Follow at compile time => compile-time ASTs!
- NOW we need to analyze grammar structure at compile time

3a. compile-time ASTs
- separate module, require at both phase 1 and phase 0
- new approch: attrs are/return compile-time ASTs, translate to exprs later!
  - tricks with prefab AST structs:
    - quasiquote and unquote
    - datum-to-expression

3b. do First/Follow
}

@; ============================================================
@; ============================================================

@;{
Beyond?
- add token information?
- add multiple backends (eg, LR(0)?)
  - split parser-specific ct data from general ct data
}
